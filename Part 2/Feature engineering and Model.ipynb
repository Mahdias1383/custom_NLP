{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div dir=\"rtl\" style=\"font-family: 'Segoe UI', Tahoma, sans-serif; font-size: 30px; color: #ffffff; font-weight: bold; padding: 10px; border-radius: 8px; background: linear-gradient(135deg, #2EA149, #60a5fa); text-align: right;\"> مهندسی ویژگی</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import textblob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "train =pd.read_csv(r\"D:\\AI Bootcamp\\Project2\\NLP\\merged_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div dir=\"rtl\" style=\"font-family: 'Segoe UI', Tahoma, sans-serif; font-size: 20px; color: #ffffff;  padding: 10px; border-radius: 8px; background: linear-gradient(135deg, #2EA149, #60a5fa); text-align: right;\"> بررسی ارتباط ستون های verified,overall, vote</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1= train.drop(columns=[ 'reviewTime', 'reviewerID', 'asin',\n",
    "       'style', 'reviewerName', 'reviewText', 'summary', 'unixReviewTime',\n",
    "       'review_length', 'title', 'brand'])\n",
    "matrix = df1.corr()\n",
    "\n",
    "# plotting correlation matrix\n",
    "plt.imshow(matrix, cmap='Blues')\n",
    "\n",
    "# adding colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# extracting variable names\n",
    "variables = []\n",
    "for i in matrix.columns:\n",
    "    variables.append(i)\n",
    "\n",
    "# Adding labels to the matrix\n",
    "plt.xticks(range(len(matrix)), variables, rotation=45, ha='right')\n",
    "plt.yticks(range(len(matrix)), variables)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div dir=\"rtl\" style=\"font-family: 'Segoe UI', Tahoma, sans-serif; font-size: 20px; color: #ffffff;  padding: 10px; border-radius: 8px; background: linear-gradient(135deg, #2EA149, #60a5fa); text-align: right;\"> به دلیل همبستگی پایین با نمره، دو ستون دیگر میتوانند از آموزش حذف شوند</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(columns=[ 'reviewTime', 'reviewerID', 'asin',\n",
    "       'style', 'reviewerName', 'unixReviewTime',\n",
    "       'review_length', 'title', 'brand','vote','verified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall        0\n",
       "reviewText     0\n",
       "summary       78\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall       0\n",
       "reviewText    0\n",
       "summary       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping null values\n",
    "train= train.dropna()\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div dir=\"rtl\" style=\"font-family: 'Segoe UI', Tahoma, sans-serif; font-size: 20px; color: #ffffff;  padding: 10px; border-radius: 8px; background: linear-gradient(135deg, #2EA149, #60a5fa); text-align: right;\"> آماده سازی توابع پردازش متن</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USER/nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed_text\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[0;32m     96\u001b[0m X \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[22], line 62\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     59\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(.)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,}\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Reduces characters occurring 3+ times to 2 (e.g., \"loooove\" -> \"loove\")\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# 5. Tokenize\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 6. Remove stopwords while keeping important ones for sentiment or in `words_to_keep`\u001b[39;00m\n\u001b[0;32m     65\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     66\u001b[0m     word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m important_stopwords \u001b[38;5;129;01mor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words_to_keep\n\u001b[0;32m     68\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\USER/nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\USER\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# before we get to feature engineering and tampering with words, we need to define the words we want to keep and contractions\n",
    "\n",
    "words_to_keep = {\n",
    "    \"good\", \"bad\", \"perfect\",'great', \"poor\",\"nice\", \"amazing\", \"disappointing\", \"excellent\", \n",
    "    \"terrible\", \"great\", \"awful\", \"best\", \"worst\", \"nice\", \"love\", \"hate\", \"confident\",\"service\",\n",
    "    \"recommend\", \"quality\", \"worth\", \"value\", \"works\", \"needs\", \"lasts\", \"ordinary\", \"successful\",\"unsuccessful\"\n",
    "    \"charges\", \"holds\", \"fits\", \"connects\", \"easy\", \"hard\", \"fast\", \"slow\", \"insult\",\n",
    "    \"durable\", \"price\", \"money\", \"buy\", \"purchase\", \"waste\", \"deal\", \"cost\", \"cancel\",\n",
    "    \"return\", \"refund\", \"replace\", \"remote\", \"tablet\", \"charger\", \"cord\", \"level\",\"fail\",\"finally\",\n",
    "    \"cable\", \"battery\", \"screen\", \"URC\", \"Roku\", \"garantee\", \"warranty\",\"common\",\"experience\"\n",
    "}\n",
    "\n",
    "contractions = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\",\n",
    "    \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\",\n",
    "    \"'ve\": \" have\", \"'m\": \" am\", \"it's\": \"it is\", \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\", \"here's\": \"here is\", \"what's\": \"what is\",\n",
    "    \"who's\": \"who is\", \"how's\": \"how is\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
    "    \"they've\": \"they have\", \"we've\": \"we have\", \"i'd\": \"i would\", \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\", \"she'd\": \"she would\", \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\", \"you'll\": \"you will\", \"he'll\": \"he will\", \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\", \"they'll\": \"they will\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\", \"hadn't\": \"had not\", \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\", \"didn't\": \"did not\", \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\", \"mightn't\": \"might not\", \"mustn't\": \"must not\"\n",
    "}\n",
    "\n",
    "# Important stopwords to retain for sentiment\n",
    "important_stopwords = {\"not\", \"no\", \"never\", \"very\", \"all\", \"any\", \"some\", \"none\", \"only\", \"too\", \"quite\", \"most\", \"more\", \"less\"}\n",
    "\n",
    "# Words indicating negation\n",
    "negation_words = {\"not\", \"no\", \"never\", \"none\", \"nothing\", \"nobody\", \"neither\", \"nowhere\", \"hardly\", \"scarcely\", \"barely\", \"without\", \"doesn’t\", \"isn’t\", \"wasn’t\", \"shouldn’t\", \"wouldn’t\", \"couldn’t\", \"don’t\", \"can’t\", \"didn’t\"}\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = train\n",
    "\n",
    "# Combine 'summary' and 'reviewText' into a single text column\n",
    "data['text'] = data['summary'].fillna('') + \" \" + data['reviewText'].fillna('')\n",
    "\n",
    "# Define preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Expand contractions\n",
    "    for contraction, expanded in contractions.items():\n",
    "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expanded, text)\n",
    "\n",
    "    # 3. Remove punctuation (keeping sentiment-relevant symbols if needed)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 4. Handle repeated characters (e.g., \"soooo good\" -> \"so good\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)  # Reduces characters occurring 3+ times to 2 (e.g., \"loooove\" -> \"loove\")\n",
    "    \n",
    "    # 5. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 6. Remove stopwords while keeping important ones for sentiment or in `words_to_keep`\n",
    "    tokens = [\n",
    "        word for word in tokens \n",
    "        if word not in stop_words or word in important_stopwords or word in words_to_keep\n",
    "    ]\n",
    "\n",
    "    # 7. Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # 8. Emphasize negations (e.g., \"not good\" becomes \"not good_NEG\")\n",
    "    emphasized_tokens = []\n",
    "    negate = False\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in negation_words:\n",
    "            negate = True\n",
    "            emphasized_tokens.append(token)\n",
    "        elif negate:\n",
    "            emphasized_tokens.append(f\"{token}_NEG\")\n",
    "            negate = False\n",
    "        else:\n",
    "            emphasized_tokens.append(token)\n",
    "\n",
    "    # 9. Rejoin tokens into processed text\n",
    "    processed_text = \" \".join(emphasized_tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply preprocessing\n",
    "data['text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Split data\n",
    "X = data['text']\n",
    "y = data['overall']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div dir=\"rtl\" style=\"font-family: 'Segoe UI', Tahoma, sans-serif; font-size: 30px; color: #ffffff; font-weight: bold; padding: 10px; border-radius: 8px; background: linear-gradient(135deg, #2EA149, #60a5fa); text-align: right;\"> مدلسازی</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div dir=\"rtl\" style=\"font-family: 'Segoe UI', Tahoma, sans-serif; font-size: 20px; color: #ffffff;  padding: 10px; border-radius: 8px; background: linear-gradient(135deg, #2EA149, #60a5fa); text-align: right;\"> مدل Logestic Regression</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Micro Averaged): 0.7103\n"
     ]
    }
   ],
   "source": [
    "# Define a pipeline with TF-IDF and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1, 3))),\n",
    "    ('clf', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=5000))\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "print(f\"F1 Score (Micro Averaged): {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall\n",
       "5    461485\n",
       "4    156514\n",
       "1     82950\n",
       "3     81239\n",
       "2     56756\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of different values in column averall\n",
    "data.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall\n",
       "1    56756\n",
       "3    56756\n",
       "5    56756\n",
       "2    56756\n",
       "4    56756\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#\n",
    "def sampling(df, num_target, each_class_size):\n",
    "    min_class_size = min(each_class_size, df[num_target].value_counts().min())\n",
    "    balanced_df = pd.concat([\n",
    "        df[df[num_target] == sentiment].sample(min_class_size, replace=True)\n",
    "        for sentiment in df[num_target].unique()\n",
    "    ])\n",
    "    return balanced_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Balance the entire dataset\n",
    "balanced_data = sampling(data, num_target='overall', each_class_size=200000)  # Set each_class_size as desired\n",
    "\n",
    "balanced_data.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Micro Averaged): 0.6050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.71      0.69     11300\n",
      "           2       0.53      0.51      0.52     11279\n",
      "           3       0.52      0.50      0.51     11491\n",
      "           4       0.58      0.55      0.56     11374\n",
      "           5       0.70      0.76      0.73     11312\n",
      "\n",
      "    accuracy                           0.61     56756\n",
      "   macro avg       0.60      0.61      0.60     56756\n",
      "weighted avg       0.60      0.61      0.60     56756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split balanced data\n",
    "X1 = balanced_data['text']\n",
    "y1 = balanced_data['overall']\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define and train pipeline\n",
    "pipeline1 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1, 3))),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', multi_class='multinomial', solver='lbfgs', max_iter=10000))\n",
    "])\n",
    "\n",
    "pipeline1.fit(X_train1, y_train1)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred1 = pipeline1.predict(X_test1)\n",
    "f1 = f1_score(y_test1, y_pred1, average='micro')\n",
    "print(f\"F1 Score (Micro Averaged): {f1:.4f}\")\n",
    "print(classification_report(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming data has already been preprocessed and split\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(data['text'], data['overall'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define pipeline with SMOTE and Logistic Regression with class weights\n",
    "pipeline2 = ImbPipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1, 2))),\n",
    "    ('smote', SMOTE(random_state=42)),  # Oversample minority classes\n",
    "    ('clf', LogisticRegression(class_weight='balanced', multi_class='multinomial', solver='lbfgs', max_iter=5000))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline2.fit(X_train2, y_train2)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred2 = pipeline2.predict(X_test2)\n",
    "f1 = f1_score(y_test2, y_pred2, average='micro')\n",
    "print(f\"F1 Score (Micro Averaged): {f1:.4f}\")\n",
    "print(classification_report(y_test2, y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom Transformer to add text length features\n",
    "class TextLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        return np.array([[len(text.split()), len(text)] for text in data])\n",
    "\n",
    "# Custom Transformer to add sentiment score\n",
    "class SentimentExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        return np.array([textblob.TextBlob(text).sentiment.polarity for text in data]).reshape(-1, 1)\n",
    "\n",
    "# Build pipeline with TF-IDF, additional features, and Gradient Boosting\n",
    "pipeline3 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1, 2), stop_words='english')),\n",
    "        ('text_length', TextLengthExtractor()),\n",
    "        ('sentiment', SentimentExtractor())\n",
    "    ])),\n",
    "    ('clf', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'features__tfidf__max_features': [10000, 20000, 30000],\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__learning_rate': [0.1, 0.05],\n",
    "    'clf__max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "# Grid search for best hyperparameters\n",
    "grid_search = GridSearchCV(pipeline3, param_grid, scoring='f1_micro', cv=3, verbose=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred3 = grid_search.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred3, average='micro')\n",
    "print(f\"Best F1 Score (Micro Averaged): {f1:.4f}\")\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a pipeline with CountVectorizer and Logistic Regression\n",
    "pipeline4 = Pipeline([\n",
    "    ('count_vec', CountVectorizer(max_features=20000, ngram_range=(1, 3))),\n",
    "    ('clf', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=5000))\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "pipeline4.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred4 = pipeline4.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred4, average='micro')\n",
    "print(f\"F1 Score (Micro Averaged): {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data.csv')\n",
    "test_data['text'] = test_data['reviewText'].fillna('').apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "test_predictions = pipeline1.predict(test_data['text'])\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "submission = pd.DataFrame({\n",
    "    'reviewerID': test_data['reviewerID'],  # Adjust this column based on test data format\n",
    "    'predicted_score': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission to a CSV file without an index, as per your requirement\n",
    "submission.to_csv('q2_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the array: 20000\n",
      "Number of unique elements: 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "size = test_predictions.size\n",
    "\n",
    "# Number of unique elements\n",
    "num_unique = len(np.unique(test_predictions))\n",
    "\n",
    "print(\"Size of the array:\", size)\n",
    "print(\"Number of unique elements:\", num_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       predicted\n",
       "0              3\n",
       "1              1\n",
       "2              2\n",
       "3              1\n",
       "4              1\n",
       "...          ...\n",
       "19995          5\n",
       "19996          5\n",
       "19997          5\n",
       "19998          2\n",
       "19999          2\n",
       "\n",
       "[20000 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a DataFrame from the predictions\n",
    "predictions_df = pd.DataFrame(test_predictions, columns=['predicted'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "predictions_df.to_csv('q2_submission.csv', index=False)\n",
    "predictions_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
